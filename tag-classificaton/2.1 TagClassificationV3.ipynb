{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c1d86f5-94fe-45e6-a0a3-94379d84d04f",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47092fb4-394e-4de1-a37b-b91ef3ee02d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q transformers[torch] accelerate\n",
    "!pip3 install -q wandb\n",
    "!pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52084bd8-b7d8-46c9-a4ac-b9befabf853d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd \n",
    "import os \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96ef8f25-49ac-4afa-adb4-5809eb7d949c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d711a1-0e5b-4d98-b300-244eb13bbaef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce7a96f4-3262-4f51-b717-9932be392b64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip3 uninstall -q wandb -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65249068-80c2-4bb8-b11f-5352cb22d0ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ce1d0b-462a-49b2-bf14-99dc9232d3d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"4de6103347df6561e7258cdef0ef60bbc1233695\", relogin=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d8c6a2e3-d432-4a39-b787-78722225a649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_DIRECTORY = \"./trained_model/v9-small\"\n",
    "DATA_DIRECTORY = \"./data/dataset/v7/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3486ece-acae-43bf-abfb-8c82bc4fec99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87883537-8e10-4469-8506-1a904cd92466",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_json(os.path.join(DATA_DIRECTORY, \"train.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7e2130c1-6ffa-4b1a-8a06-0ad1e75d63ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# print(train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8f5fd7d-4a76-4277-8bc4-c3dcb2335d85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = train_df['section_content'].values\n",
    "labels_text = train_df['tags'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "46f22f3c-5405-4dbb-a34e-894c7c55d54b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd6525f5-779c-448c-a563-71f7744afc1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c63686c3-265f-443e-9d5b-6de0761573d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIRECTORY, \"tag.le\"), 'wb') as file: \n",
    "    pickle.dump(label_encoder, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c24ec504-b16a-44ab-be65-710d82a51216",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels 23\n"
     ]
    }
   ],
   "source": [
    "num_labels = train_df.tags.unique().shape[0]\n",
    "print(\"Number of labels\", num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5f6c7-39f5-437c-b9d7-f27320c036ad",
   "metadata": {},
   "source": [
    "## Calculate Class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28a404dd-1cf2-4015-807e-473dd5611f20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "397272fd-3000-4e12-9db7-94a38e83040e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Weights:  {0: 2.2355072463768115, 1: 3.8322981366459627, 2: 1.889161053276179, 3: 3.945012787723785, 4: 2.273397199705232, 5: 1.166351606805293, 6: 2.1633941093969145, 7: 1.6160293347302253, 8: 2.6826086956521737, 9: 3.945012787723785, 10: 1.8125734430082256, 11: 2.0635451505016724, 12: 2.353165522501907, 13: 0.07037273598248095, 14: 7.451690821256038, 15: 3.8322981366459627, 16: 2.0635451505016724, 17: 2.9158790170132325, 18: 1.8125734430082256, 19: 7.059496567505721, 20: 3.945012787723785, 21: 3.6251468860164513, 22: 2.57943143812709}\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(labels), y=labels)\n",
    "class_weights_dict = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "print(\"Class Weights: \", class_weights_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bdd9ba53-ddc7-4a88-ab17-0458337a39b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_weight_tensor = torch.tensor(class_weights, dtype=torch.float) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9724721d-a2eb-4fe3-b201-f6ded2967bfc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_weighted_loss(model, inputs, return_outputs=False): \n",
    "    labels = inputs.get(\"labels\") \n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.get(\"logits\")\n",
    "    loss_fct = torch.nn.CrossEntropyLoss(weight=class_weight_tensor.to(logits.device))\n",
    "    loss = loss_fct(logits, labels) \n",
    "    return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "30342f87-9960-4663-9e39-d125d17c5bcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_name = 'bert-base-uncased'\n",
    "# tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "# model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f96031-b9da-4c2a-8e8d-5840325cd917",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cd9bf7a-ffd6-4d53-8f03-c54f23b6e18d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d955393-a070-4bd9-af03-d1da25c3997d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d9c6a50a-3f82-400c-99b0-09e3b8ed5302",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = TextDataset(texts, labels, tokenizer, max_length=512)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cf9cbe-89cc-492b-a17d-ef09b4125c0c",
   "metadata": {},
   "source": [
    "## Compute Metrics callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3bf0a4f0-0423-4023-ac89-b8c11ef65cfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e27fa414-69a1-4ee4-84d9-cf20990625c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    accuracy = accuracy_score(labels, predictions, )\n",
    "    precision = precision_score(labels, predictions, average='macro')\n",
    "    recall = recall_score(labels, predictions, average='macro')\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60d4033d-0c64-4e80-ba0a-fb49aee49d02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./trained_model/v9-small'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_DIRECTORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c4f29c1-cb9e-47d0-b7d3-619b4fef4b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, class_weights=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.get(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        loss_fct = torch.nn.CrossEntropyLoss(weight=self.class_weights.to(logits.device))\n",
    "        loss = loss_fct(logits, labels)\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ccb478e-258b-4078-b195-0c9ca32d80c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2427982d-681b-487c-aa59-e818d637cf73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=os.path.join(MODEL_DIRECTORY, './results'),\n",
    "    run_name=f\"{Path(MODEL_DIRECTORY).name}\",\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=os.path.join(MODEL_DIRECTORY, './logs'),\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-5,\n",
    "    eval_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    "    eval_dataset=dataset, \n",
    "    compute_metrics=compute_metrics, \n",
    "    class_weights=class_weight_tensor\n",
    ")\n",
    "\n",
    "# Train the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ca020a-9dd4-4d0d-9272-928665e56495",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manurag-credcore\u001b[0m (\u001b[33mcredcore\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/root/wandb/run-20240821_065357-1r5nbmiw</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/credcore/huggingface/runs/1r5nbmiw' target=\"_blank\">v9-small</a></strong> to <a href='https://wandb.ai/credcore/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/credcore/huggingface' target=\"_blank\">https://wandb.ai/credcore/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/credcore/huggingface/runs/1r5nbmiw' target=\"_blank\">https://wandb.ai/credcore/huggingface/runs/1r5nbmiw</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4363' max='9650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4363/9650 50:14 < 1:00:54, 1.45 it/s, Epoch 22.60/50]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.725200</td>\n",
       "      <td>2.175436</td>\n",
       "      <td>0.534522</td>\n",
       "      <td>0.522477</td>\n",
       "      <td>0.838442</td>\n",
       "      <td>0.603741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.756600</td>\n",
       "      <td>0.908236</td>\n",
       "      <td>0.847326</td>\n",
       "      <td>0.753365</td>\n",
       "      <td>0.957801</td>\n",
       "      <td>0.830798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.175800</td>\n",
       "      <td>0.420771</td>\n",
       "      <td>0.901135</td>\n",
       "      <td>0.809032</td>\n",
       "      <td>0.985435</td>\n",
       "      <td>0.883094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.080100</td>\n",
       "      <td>0.146331</td>\n",
       "      <td>0.962723</td>\n",
       "      <td>0.915118</td>\n",
       "      <td>0.980429</td>\n",
       "      <td>0.944691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.029000</td>\n",
       "      <td>0.128852</td>\n",
       "      <td>0.971799</td>\n",
       "      <td>0.928795</td>\n",
       "      <td>0.994975</td>\n",
       "      <td>0.959055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>0.109850</td>\n",
       "      <td>0.978606</td>\n",
       "      <td>0.949820</td>\n",
       "      <td>0.996710</td>\n",
       "      <td>0.971821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.216300</td>\n",
       "      <td>0.096705</td>\n",
       "      <td>0.983793</td>\n",
       "      <td>0.960313</td>\n",
       "      <td>0.995266</td>\n",
       "      <td>0.976844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.013100</td>\n",
       "      <td>0.079971</td>\n",
       "      <td>0.982820</td>\n",
       "      <td>0.957836</td>\n",
       "      <td>0.993618</td>\n",
       "      <td>0.974326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.016900</td>\n",
       "      <td>0.054256</td>\n",
       "      <td>0.988979</td>\n",
       "      <td>0.971061</td>\n",
       "      <td>0.996876</td>\n",
       "      <td>0.983388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.009400</td>\n",
       "      <td>0.051726</td>\n",
       "      <td>0.991248</td>\n",
       "      <td>0.974612</td>\n",
       "      <td>0.998819</td>\n",
       "      <td>0.986109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.006800</td>\n",
       "      <td>0.043534</td>\n",
       "      <td>0.992220</td>\n",
       "      <td>0.978136</td>\n",
       "      <td>0.997632</td>\n",
       "      <td>0.987433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.004500</td>\n",
       "      <td>0.048608</td>\n",
       "      <td>0.991248</td>\n",
       "      <td>0.976633</td>\n",
       "      <td>0.998819</td>\n",
       "      <td>0.987258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.041956</td>\n",
       "      <td>0.992545</td>\n",
       "      <td>0.977507</td>\n",
       "      <td>0.998911</td>\n",
       "      <td>0.987630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.004700</td>\n",
       "      <td>0.022133</td>\n",
       "      <td>0.995462</td>\n",
       "      <td>0.985596</td>\n",
       "      <td>0.998551</td>\n",
       "      <td>0.991764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.004300</td>\n",
       "      <td>0.022497</td>\n",
       "      <td>0.995462</td>\n",
       "      <td>0.986005</td>\n",
       "      <td>0.999116</td>\n",
       "      <td>0.992237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.003200</td>\n",
       "      <td>0.024881</td>\n",
       "      <td>0.994489</td>\n",
       "      <td>0.986289</td>\n",
       "      <td>0.994924</td>\n",
       "      <td>0.990195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.032200</td>\n",
       "      <td>0.015734</td>\n",
       "      <td>0.996110</td>\n",
       "      <td>0.985212</td>\n",
       "      <td>0.998597</td>\n",
       "      <td>0.991089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.001900</td>\n",
       "      <td>0.021132</td>\n",
       "      <td>0.996110</td>\n",
       "      <td>0.988597</td>\n",
       "      <td>0.997906</td>\n",
       "      <td>0.993073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.012088</td>\n",
       "      <td>0.996759</td>\n",
       "      <td>0.989740</td>\n",
       "      <td>0.997951</td>\n",
       "      <td>0.993574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.004000</td>\n",
       "      <td>0.009437</td>\n",
       "      <td>0.997731</td>\n",
       "      <td>0.993085</td>\n",
       "      <td>0.999276</td>\n",
       "      <td>0.996094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.042200</td>\n",
       "      <td>0.005606</td>\n",
       "      <td>0.998379</td>\n",
       "      <td>0.995562</td>\n",
       "      <td>0.999321</td>\n",
       "      <td>0.997397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.000700</td>\n",
       "      <td>0.003285</td>\n",
       "      <td>0.999028</td>\n",
       "      <td>0.998217</td>\n",
       "      <td>0.998802</td>\n",
       "      <td>0.998495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1936\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1938\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1939\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:2236\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2233\u001b[0m     rng_to_sync \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   2235\u001b[0m step \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 2236\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(epoch_iterator):\n\u001b[1;32m   2237\u001b[0m     total_batched_samples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   2239\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39minclude_num_input_tokens_seen:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/accelerate/data_loader.py:464\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    462\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    463\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m--> 464\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     49\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     12\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtexts[idx]\n\u001b[1;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels[idx]\n\u001b[0;32m---> 14\u001b[0m encoding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m     21\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m: encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mflatten(),\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m: torch\u001b[38;5;241m.\u001b[39mtensor(labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     26\u001b[0m }\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3237\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3228\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3229\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3230\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3234\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3235\u001b[0m )\n\u001b[0;32m-> 3237\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3239\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3240\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3241\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3242\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3247\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3248\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3249\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3251\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3255\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3256\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3257\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:797\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_offsets_mapping:\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_offset_mapping is not available when using Python tokenizers. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo use this feature, change your tokenizer to one deriving from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://github.com/huggingface/transformers/pull/2674\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    795\u001b[0m     )\n\u001b[0;32m--> 797\u001b[0m first_ids \u001b[38;5;241m=\u001b[39m \u001b[43mget_input_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    798\u001b[0m second_ids \u001b[38;5;241m=\u001b[39m get_input_ids(text_pair) \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_for_model(\n\u001b[1;32m    801\u001b[0m     first_ids,\n\u001b[1;32m    802\u001b[0m     pair_ids\u001b[38;5;241m=\u001b[39msecond_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    816\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    817\u001b[0m )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:764\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._encode_plus.<locals>.get_input_ids\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_input_ids\u001b[39m(text):\n\u001b[1;32m    763\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m--> 764\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    765\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(tokens)\n\u001b[1;32m    766\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(text) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(text[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils.py:695\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.tokenize\u001b[0;34m(self, text, **kwargs)\u001b[0m\n\u001b[1;32m    693\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mappend(token)\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 695\u001b[0m         tokenized_text\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    696\u001b[0m \u001b[38;5;66;03m# [\"This\", \" is\", \" something\", \"<special_token_1>\", \"else\"]\u001b[39;00m\n\u001b[1;32m    697\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_text\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py:162\u001b[0m, in \u001b[0;36mDistilBertTokenizer._tokenize\u001b[0;34m(self, text, split_special_tokens)\u001b[0m\n\u001b[1;32m    160\u001b[0m split_tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdo_basic_tokenize:\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbasic_tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnever_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    164\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;66;03m# If the token is part of the never_split set\u001b[39;00m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbasic_tokenizer\u001b[38;5;241m.\u001b[39mnever_split:\n\u001b[1;32m    167\u001b[0m             split_tokens\u001b[38;5;241m.\u001b[39mappend(token)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py:357\u001b[0m, in \u001b[0;36mBasicTokenizer.tokenize\u001b[0;34m(self, text, never_split)\u001b[0m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# This was added on November 1st, 2018 for the multilingual and Chinese\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# models. This is also applied to the English models now, but it doesn't\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# matter since the English models were not trained on any Chinese data\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;66;03m# and generally don't have any Chinese data in them (there are Chinese\u001b[39;00m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;66;03m# characters in the vocabulary because Wikipedia does have some Chinese\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;66;03m# words in the English Wikipedia.).\u001b[39;00m\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenize_chinese_chars:\n\u001b[0;32m--> 357\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tokenize_chinese_chars\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# prevents treating the same character with different unicode codepoints as different characters\u001b[39;00m\n\u001b[1;32m    359\u001b[0m unicode_normalized_text \u001b[38;5;241m=\u001b[39m unicodedata\u001b[38;5;241m.\u001b[39mnormalize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNFC\u001b[39m\u001b[38;5;124m\"\u001b[39m, text)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py:413\u001b[0m, in \u001b[0;36mBasicTokenizer._tokenize_chinese_chars\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m char \u001b[38;5;129;01min\u001b[39;00m text:\n\u001b[1;32m    412\u001b[0m     cp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mord\u001b[39m(char)\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_chinese_char\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcp\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    414\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    415\u001b[0m         output\u001b[38;5;241m.\u001b[39mappend(char)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/distilbert/tokenization_distilbert.py:432\u001b[0m, in \u001b[0;36mBasicTokenizer._is_chinese_char\u001b[0;34m(self, cp)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Checks whether CP is the codepoint of a CJK character.\"\"\"\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;66;03m# This defines a \"chinese character\" as anything in the CJK Unicode block:\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[38;5;66;03m#   https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;66;03m# space-separated words, so they are not treated specially and handled\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;66;03m# like the all of the other languages.\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m--> 432\u001b[0m     (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x4E00\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x9FFF\u001b[39m)\n\u001b[1;32m    433\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x3400\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x4DBF\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    434\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x20000\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2A6DF\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2A700\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2B73F\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2B740\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2B81F\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2B820\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2CEAF\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    438\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0xF900\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0xFAFF\u001b[39m)\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m (cp \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2F800\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m cp \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0x2FA1F\u001b[39m)  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    440\u001b[0m ):  \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0f4ca592-10ac-4df1-a926-6344f93b5239",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# eval_results = trainer.evaluate()\n",
    "# print(f\"Evaluation results: {eval_results}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e9e6351b-6ac4-457a-8f70-f30cfa931f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_model/v9-small/tokenizer_config.json',\n",
       " './trained_model/v9-small/special_tokens_map.json',\n",
       " './trained_model/v9-small/vocab.txt',\n",
       " './trained_model/v9-small/added_tokens.json')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the model and tokenizer to disk\n",
    "model.save_pretrained(MODEL_DIRECTORY)\n",
    "tokenizer.save_pretrained(MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32cdfca5-ae47-43ad-aea0-50aa126fc3a4",
   "metadata": {},
   "source": [
    "# Load Model For Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67866497-5cea-4bc1-bd15-4b900076d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers[torch] accelerate\n",
    "!pip3 install -q wandb\n",
    "!pip3 install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35845363-4928-4238-bacd-f428fe543da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DIRECTORY = \"./model\" # latest deployed model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9e93f17-03d0-4590-8139-854601ed90cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json \n",
    "import pandas as pd \n",
    "import os \n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ace0352-1738-4ad3-b0d8-5129f6113856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "40556c6a-45c9-402c-95d0-4b171e543866",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import classification_report, precision_recall_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16729842-6cbe-494a-8d09-ab25fb29c3c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./trained_model/v9-small\n"
     ]
    }
   ],
   "source": [
    "print(MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64cbd56b-f7b2-4f45-9fcc-690bee000fa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_DIRECTORY)\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_DIRECTORY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15cf562f-3454-437e-9f9e-be23ad3839ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_json(os.path.join(DATA_DIRECTORY, \"test.json\"))\n",
    "test_df = test_df.drop_duplicates(\"section_content\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a6df379-76a6-477e-99d1-3e0d556197cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(os.path.join(MODEL_DIRECTORY, \"tag.le\"), 'rb') as file: \n",
    "    label_encoder = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af0e5cb4-fa72-4b18-8135-bf439755836f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_texts = test_df['section_content'].values\n",
    "test_labels = test_df['tags'].values\n",
    "test_labels = label_encoder.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af939265-abf3-408c-8b2b-1251be41c691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "eeb0dfa9-a95e-4d6f-be3f-c947b1e97e30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        labels = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d42d421c-2740-4f61-acfb-755046c2d734",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dataset = TextDataset(test_texts, test_labels, tokenizer, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd09a4a9-c137-449e-9289-3c1b582ebd35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "58c83f58-de69-4597-8cf1-3f6f81a78a33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_args = TrainingArguments(\n",
    "    output_dir='./results',  \n",
    "    per_device_eval_batch_size=32, \n",
    "    dataloader_drop_last=False,  \n",
    "    no_cuda=False if torch.cuda.is_available() else True, \n",
    "    seed=42,  \n",
    "    report_to=\"none\" # Disable wandb reporting. \n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    args = inference_args\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b594fedc-129a-4cfc-a175-141abe203221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c540d157-2208-4b7a-85cf-161b46ce089a",
   "metadata": {},
   "source": [
    "## Training Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c81a6c0b-c6e8-4280-9127-1d191245ae3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Make predictions on the test set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m predictions \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mpredict(\u001b[43mdataset\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Extract the logits and convert to predicted labels\u001b[39;00m\n\u001b[1;32m      5\u001b[0m logits \u001b[38;5;241m=\u001b[39m predictions\u001b[38;5;241m.\u001b[39mpredictions\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = trainer.predict(dataset)\n",
    "\n",
    "# Extract the logits and convert to predicted labels\n",
    "logits = predictions.predictions\n",
    "predicted_labels = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Convert numeric labels back to original text labels for comparison\n",
    "predicted_labels_text = label_encoder.inverse_transform(predicted_labels)\n",
    "true_labels_text = label_encoder.inverse_transform(labels)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(true_labels_text, predicted_labels_text, target_names=label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce71398d-9a20-4ec3-b036-8a16e595122e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                  Additional Liens       1.00      1.00      1.00        60\n",
      "                 Asset Disposition       1.00      1.00      1.00        35\n",
      "            Compliance Certificate       1.00      1.00      1.00        71\n",
      "           Consequences of Default       0.94      1.00      0.97        34\n",
      "                  Event of Default       1.00      1.00      1.00        59\n",
      "           Facilities / Instrument       1.00      1.00      1.00       115\n",
      "                Financial Covenant       1.00      1.00      1.00        62\n",
      "              Financial Statements       0.99      1.00      0.99        83\n",
      "                    Governing Laws       1.00      1.00      1.00        50\n",
      "            Incremental Facilities       1.00      1.00      1.00        34\n",
      "                     Interest Rate       1.00      0.99      0.99        74\n",
      "                    Loan Repayment       1.00      1.00      1.00        65\n",
      "Mandatory Prepayments / Redemption       1.00      1.00      1.00        57\n",
      "                                NA       1.00      1.00      1.00      1906\n",
      "  Optional Prepayment / Redemption       0.95      1.00      0.97        18\n",
      "            Permitted Indebtedness       1.00      1.00      1.00        35\n",
      "                  Premium and Fees       1.00      1.00      1.00        65\n",
      "                        Prepayment       1.00      1.00      1.00        46\n",
      "            Reporting Requirements       1.00      0.99      0.99        74\n",
      "            Restricted Investments       1.00      1.00      1.00        19\n",
      "               Restricted Payments       1.00      1.00      1.00        34\n",
      "      Transactions with Affiliates       1.00      1.00      1.00        37\n",
      "             Waterfall of Proceeds       1.00      1.00      1.00        52\n",
      "\n",
      "                          accuracy                           1.00      3085\n",
      "                         macro avg       0.99      1.00      1.00      3085\n",
      "                      weighted avg       1.00      1.00      1.00      3085\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1d2105-0d86-442d-8144-2838906f03b9",
   "metadata": {},
   "source": [
    "## Test Accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d81d8d2b-24dd-465c-86a2-3e547664feba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = trainer.predict(test_dataset)\n",
    "\n",
    "# Extract the logits and convert to predicted labels\n",
    "logits = predictions.predictions\n",
    "predicted_labels = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Convert numeric labels back to original text labels for comparison\n",
    "predicted_labels_text = label_encoder.inverse_transform(predicted_labels)\n",
    "true_labels_text = label_encoder.inverse_transform(test_labels)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(true_labels_text, predicted_labels_text, target_names=label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd4c0bc7-ddd2-41d8-8113-1773e63995b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                  Additional Liens       1.00      0.94      0.97        18\n",
      "                 Asset Disposition       0.86      1.00      0.92        12\n",
      "            Compliance Certificate       0.67      0.67      0.67         3\n",
      "           Consequences of Default       0.71      0.91      0.80        11\n",
      "                  Event of Default       1.00      1.00      1.00        13\n",
      "           Facilities / Instrument       0.90      0.90      0.90        20\n",
      "                Financial Covenant       0.92      1.00      0.96        12\n",
      "              Financial Statements       0.57      1.00      0.73         4\n",
      "                    Governing Laws       1.00      0.91      0.95        11\n",
      "            Incremental Facilities       1.00      0.83      0.91         6\n",
      "                     Interest Rate       0.85      0.89      0.87        19\n",
      "                    Loan Repayment       1.00      0.93      0.97        15\n",
      "Mandatory Prepayments / Redemption       1.00      1.00      1.00         1\n",
      "                                NA       0.99      0.97      0.98       526\n",
      "  Optional Prepayment / Redemption       1.00      1.00      1.00         8\n",
      "            Permitted Indebtedness       1.00      1.00      1.00         9\n",
      "                  Premium and Fees       0.93      1.00      0.96        13\n",
      "                        Prepayment       1.00      1.00      1.00        10\n",
      "            Reporting Requirements       0.94      0.83      0.88        18\n",
      "            Restricted Investments       1.00      1.00      1.00         4\n",
      "               Restricted Payments       0.89      1.00      0.94         8\n",
      "      Transactions with Affiliates       1.00      1.00      1.00        11\n",
      "             Waterfall of Proceeds       0.79      0.92      0.85        12\n",
      "\n",
      "                          accuracy                           0.96       764\n",
      "                         macro avg       0.91      0.94      0.92       764\n",
      "                      weighted avg       0.97      0.96      0.96       764\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f6a22185-999d-4127-9539-2ede8d193168",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df['Predicted_Tag'] = predicted_labels_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d3cad14-e9e0-4ada-8dd6-0a75c1478e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = test_df.rename(columns = {\"tag\": \"Original_Tag\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3755ef69-2dae-4144-99ae-12203f2520a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(764, 9)\n"
     ]
    }
   ],
   "source": [
    "print(test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c94fd4e1-d5e5-44f5-a1c8-100fbb4f4993",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.to_excel(\"./data/output/predictions_v9_small.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b834fa43-41bb-4055-8387-2debcd29e93c",
   "metadata": {},
   "source": [
    "## Generate Results on Unsampled NA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "79155d24-f0e9-4b85-898d-65c626e35c08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "seen_df = pd.read_json(\"./data/cleaned_tags_data.json\")\n",
    "all_df = pd.read_json(\"./data/all-data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c2cf7e89-9050-4b6e-b588-da7e3e27f1e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unseen_df = all_df.merge(seen_df, how='left', on=['filename', 'tags', 'title', 'category', 'word_count', 'section_content'], indicator=True, )\n",
    "unseen_df = unseen_df[unseen_df['_merge'] == 'left_only']\n",
    "unseen_df = unseen_df.drop(columns=['_merge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9532b7f8-ba2e-4f4c-b30b-a297f9496021",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "unseen_df['tags'] = unseen_df['tags'].str.replace(\"Events of Default\", \"Event of Default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8f9fcc85-b1c8-4ac7-b726-c897d1a70b8f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unseen_texts = unseen_df['section_content'].values\n",
    "unseen_labels = unseen_df['tags'].values\n",
    "unseen_labels = label_encoder.transform(unseen_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "300a27ee-844a-4883-80f2-edd744ce59da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unseen_dataset = TextDataset(unseen_texts, unseen_labels, tokenizer, max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8ff28a86-4987-48d2-8d7f-f2bec7170ed2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make predictions on the test set\n",
    "predictions = trainer.predict(unseen_dataset)\n",
    "\n",
    "# Extract the logits and convert to predicted labels\n",
    "logits = predictions.predictions\n",
    "predicted_labels = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Convert numeric labels back to original text labels for comparison\n",
    "predicted_labels_text = label_encoder.inverse_transform(predicted_labels)\n",
    "true_labels_text = label_encoder.inverse_transform(unseen_labels)\n",
    "\n",
    "# Generate a classification report\n",
    "report = classification_report(true_labels_text, predicted_labels_text, target_names=label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3e56f6df-0794-42cb-98fa-5dc4136c9c16",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    precision    recall  f1-score   support\n",
      "\n",
      "                  Additional Liens       0.51      0.93      0.66        29\n",
      "                 Asset Disposition       0.41      0.83      0.55        18\n",
      "            Compliance Certificate       0.50      1.00      0.67        14\n",
      "           Consequences of Default       0.45      0.26      0.33        92\n",
      "                  Event of Default       0.83      0.64      0.73       107\n",
      "           Facilities / Instrument       0.17      0.97      0.30        36\n",
      "                Financial Covenant       0.53      0.75      0.62        24\n",
      "              Financial Statements       0.07      1.00      0.13         1\n",
      "                    Governing Laws       0.46      0.96      0.62        51\n",
      "            Incremental Facilities       0.17      1.00      0.29         8\n",
      "                     Interest Rate       0.42      0.95      0.59        44\n",
      "                    Loan Repayment       0.44      0.93      0.60        29\n",
      "Mandatory Prepayments / Redemption       0.64      1.00      0.78        16\n",
      "                                NA       0.99      0.92      0.95      9951\n",
      "  Optional Prepayment / Redemption       0.67      0.95      0.78        21\n",
      "            Permitted Indebtedness       0.46      0.89      0.61        19\n",
      "                  Premium and Fees       0.52      0.75      0.61        20\n",
      "                        Prepayment       0.33      0.75      0.46         8\n",
      "            Reporting Requirements       0.29      0.61      0.40        33\n",
      "            Restricted Investments       0.60      0.90      0.72        10\n",
      "               Restricted Payments       0.35      0.79      0.48        19\n",
      "      Transactions with Affiliates       0.48      0.93      0.64        15\n",
      "             Waterfall of Proceeds       0.12      0.88      0.21        24\n",
      "\n",
      "                          accuracy                           0.91     10589\n",
      "                         macro avg       0.45      0.85      0.55     10589\n",
      "                      weighted avg       0.96      0.91      0.93     10589\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4374cd9-18d1-4374-b4b1-1171a387a214",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a60909-9c19-4e80-b657-65ae01442403",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_df = pd.read_json(\"./data/all-data.json\")\n",
    "print(all_df.shape)\n",
    "all_df = all_df[all_df.word_count > 20]\n",
    "print(all_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5362c67f-3a91-4011-9e98-9274afc18178",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f6f60-04dc-41a8-8096-e2891d12e019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7240c4e5-3acb-4176-a8de-a09e9e98197a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df['predicted_tags'] = predicted_labels_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133053cb-d347-426f-9ec5-f364beb2fbdc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0534d6c-cb0a-4932-b35b-8320c1a63c28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "group_df = all_df[['section_content', 'tags']].groupby('section_content').agg(list).reset_index()\n",
    "group_df['n_tags'] = group_df.tags.apply(lambda x: len(x))\n",
    "group_df['n_unique_tags'] = group_df.tags.apply(lambda x: len(set(x)))\n",
    "print(group_df[group_df.n_tags > 1].shape)\n",
    "print(group_df[group_df.n_unique_tags > 1].shape)\n",
    "group_df.sort_values(by='n_unique_tags', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38bb147-4b79-40a4-a043-bf48ebd03863",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e563a-72fc-4ed8-9bf5-dadf9a41cdd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df[[\"section_content\", \"predicted_tags\"]].merge(all_df.drop_duplicates(\"section_content\"), how=\"left\", on=\"section_content\", validate=\"one_to_one\").to_excel(\"./data/output/test_data_predictions.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11211d80-d757-4a9d-92d3-9292644ba218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d48248-8dd6-4b5e-892a-8f6c86535fcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92c491b-9b0f-4b6d-8e16-c3519c5b3cc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def predict_labels(texts, model, tokenizer, label_encoder, max_length=512, device='cuda'):\n",
    "    # Move model to the specified device\n",
    "    model.to(device)\n",
    "    \n",
    "    # Tokenize the texts\n",
    "    encodings = tokenizer(\n",
    "        texts,\n",
    "        max_length=max_length,\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Move tensors to the same device as the model\n",
    "    input_ids = encodings['input_ids'].to(device)\n",
    "    attention_mask = encodings['attention_mask'].to(device)\n",
    "    \n",
    "    # Put the model in evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Perform the inference\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        logits = outputs.logits\n",
    "        predictions = torch.argmax(logits, dim=-1)\n",
    "    \n",
    "    # Move predictions back to the CPU before converting to numpy\n",
    "    predicted_labels = label_encoder.inverse_transform(predictions.cpu().numpy())\n",
    "    \n",
    "    return predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b037728-c863-4e9b-ba48-d7aafeca33aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_labels_text = predict_labels(test_texts.tolist(), model, tokenizer, label_encoder)\n",
    "true_labels_text = label_encoder.inverse_transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a924cab-231b-4ecc-8869-3f69b492374e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630ebe6a-4610-49bc-b085-7168ff4b2a6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Extract the logits and convert to predicted labels\n",
    "# logits = predictions.predictions\n",
    "# predicted_labels = np.argmax(logits, axis=-1)\n",
    "\n",
    "# Convert numeric labels back to original text labels for comparison\n",
    "# Generate a classification report\n",
    "report = classification_report(true_labels_text, predicted_labels_text, target_names=label_encoder.classes_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af914524-1988-4f1f-9c94-47aa47e3ec47",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(report)"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 2.0.0 Python 3.10 GPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-2.0.0-gpu-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
